{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b594c3",
   "metadata": {},
   "source": [
    "## Cristiano Nicolau - 108536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "852d2c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_set, size, explode\n",
    "from itertools import combinations\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ccefcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 16:27:07 WARN Utils: Your hostname, cristianonicolau.local resolves to a loopback address: 127.0.0.1; using 192.168.0.3 instead (on interface en0)\n",
      "25/04/30 16:27:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/30 16:27:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+--------------------+---------+--------------------+\n",
      "|     START|      STOP|             PATIENT|           ENCOUNTER|     CODE|         DESCRIPTION|\n",
      "+----------+----------+--------------------+--------------------+---------+--------------------+\n",
      "|2017-01-14|2017-03-30|09e4e8cb-29c2-4ef...|88e540ab-a7d7-47d...| 65363002|        Otitis media|\n",
      "|2012-09-15|2012-09-16|b0a03e8c-8d0f-424...|e89414dc-d0c6-478...|241929008|Acute allergic re...|\n",
      "|2018-06-17|2018-06-24|09e4e8cb-29c2-4ef...|c14325b0-f7ec-431...|444814009|Viral sinusitis (...|\n",
      "+----------+----------+--------------------+--------------------+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sp = SparkSession.builder.appName(\"AprioriConditions\").getOrCreate()\n",
    "df = sp.read.csv(\"./_input/conditions.csv.gz\", header=True, inferSchema=True)\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b6616e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeiro elemento:\n",
      "[703151001, 128613002, 70704007, 444814009, 65363002, 232353008, 192127007, 195662009]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "patient_condition_lists = df.groupBy(\"PATIENT\").agg(collect_set(\"CODE\").alias(\"conditions\")).select(\"conditions\").rdd.map(lambda row: row.conditions)\n",
    "first_row = patient_condition_lists.first()\n",
    "print(\"Primeiro elemento:\")\n",
    "print(first_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "639aae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de condições:\n",
      "1157578\n"
     ]
    }
   ],
   "source": [
    "transactions_list = patient_condition_lists.collect()\n",
    "transaction_count = len(transactions_list)\n",
    "\n",
    "print(\"Total de condições:\")\n",
    "print(transaction_count)\n",
    "\n",
    "min_support = 1000\n",
    "min_support_ratio = min_support / transaction_count\n",
    "\n",
    "min_standardized_lift = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c3f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 frequent 1-itemsets, support:\n",
      "(703151001,) 0.03688131598907374\n",
      "(128613002,) 0.03688131598907374\n",
      "(70704007,) 0.04510020059123446\n",
      "(444814009,) 0.6495804170431712\n",
      "(65363002,) 0.11577448776669909\n",
      "Total frequent 1-itemsets:\n",
      "131\n"
     ]
    }
   ],
   "source": [
    "item_counts = {}\n",
    "for transaction in transactions_list:\n",
    "    for item in transaction:\n",
    "        item_counts[item] = item_counts.get(item, 0) + 1\n",
    "\n",
    "frequent_1_itemsets = []\n",
    "support_data = {}\n",
    "\n",
    "# Compute support and filter by min_support\n",
    "for item, count in item_counts.items():\n",
    "    support = count / transaction_count\n",
    "    if support >= min_support_ratio:\n",
    "        itemset = (item,)\n",
    "        frequent_1_itemsets.append(itemset)\n",
    "        support_data[itemset] = support\n",
    "\n",
    "frequent_items = frequent_1_itemsets\n",
    "all_frequent_itemsets = frequent_1_itemsets[:]\n",
    "\n",
    "print(\"Top 5 frequent 1-itemsets, support:\")\n",
    "for itemset in frequent_items[:5]:\n",
    "    print(itemset, support_data[itemset])\n",
    "\n",
    "print(\"Total frequent 1-itemsets:\")\n",
    "print(len(frequent_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd199ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_gen(frequent_prev, k):\n",
    "    candidates = set()\n",
    "    for a, b in combinations(frequent_prev, 2):\n",
    "        union = tuple(sorted(set(a).union(set(b))))\n",
    "        if len(union) == k:\n",
    "            candidates.add(union)\n",
    "    return list(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d839459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_k2, top_k3 = [], []\n",
    "\n",
    "for k in [2, 3]:\n",
    "    candidates = apriori_gen(frequent_items, k)\n",
    "    candidate_rdd = sp.sparkContext.parallelize(candidates)\n",
    "\n",
    "    def count_support(candidate):\n",
    "        count = 0\n",
    "        for t in transactions_list:\n",
    "            if all(item in t for item in candidate):\n",
    "                count += 1\n",
    "        return (candidate, count)\n",
    "\n",
    "    counted = candidate_rdd.map(count_support).filter(lambda x: x[1] >= min_support).collect()\n",
    "\n",
    "    support_data.update({itemset: count / transaction_count for itemset, count in counted})\n",
    "    frequent_k = [itemset for itemset, _ in counted]\n",
    "    all_frequent_itemsets.extend(frequent_k)\n",
    "    frequent_items = frequent_k\n",
    "\n",
    "    if k == 2:\n",
    "        top_k2 = sorted(counted, key=lambda x: x[1], reverse=True)[:10]\n",
    "    elif k == 3:\n",
    "        top_k3 = sorted(counted, key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f02c403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 frequent itemsets for k=2:\n",
      "{195662009, 444814009} support: 343651\n",
      "{444814009, 10509002} support: 302516\n",
      "{15777000, 271737000} support: 289176\n",
      "{444814009, 162864005} support: 243812\n",
      "{271737000, 444814009} support: 236847\n",
      "{15777000, 444814009} support: 236320\n",
      "{195662009, 10509002} support: 211065\n",
      "{59621000, 444814009} support: 203450\n",
      "{195662009, 162864005} support: 167438\n",
      "{40055000, 444814009} support: 165530\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 frequent itemsets for k=2:\")\n",
    "for itemset, count in top_k2:\n",
    "    print(set(itemset), \"support:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ed8106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 frequent itemsets for k=3:\n",
      "{15777000, 444814009, 271737000} support: 192819\n",
      "{195662009, 10509002, 444814009} support: 139174\n",
      "{15777000, 195662009, 271737000} support: 132583\n",
      "{15777000, 10509002, 271737000} support: 115510\n",
      "{195662009, 444814009, 162864005} support: 111860\n",
      "{271737000, 195662009, 444814009} support: 108560\n",
      "{15777000, 195662009, 444814009} support: 108083\n",
      "{15777000, 59621000, 271737000} support: 99818\n",
      "{444814009, 10509002, 162864005} support: 97384\n",
      "{271737000, 444814009, 10509002} support: 94793\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 10 frequent itemsets for k=3:\")\n",
    "for itemset, count in top_k3:\n",
    "    print(set(itemset), \"support:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "674f2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rules(frequent_itemsets, support_data):\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "        if len(itemset) >= 2:\n",
    "            for i in range(1, len(itemset)):\n",
    "                for antecedent in combinations(itemset, i):\n",
    "                    consequent = tuple(sorted(set(itemset) - set(antecedent)))\n",
    "                    antecedent = tuple(sorted(antecedent))\n",
    "                    if not consequent:\n",
    "                        continue\n",
    "                    support_itemset = support_data.get(tuple(sorted(itemset)))\n",
    "                    support_antecedent = support_data.get(tuple(sorted(antecedent)))\n",
    "                    support_consequent = support_data.get(tuple(sorted(consequent)))\n",
    "                    if not (support_itemset and support_antecedent and support_consequent):\n",
    "                        continue\n",
    "\n",
    "                    confidence = support_itemset / support_antecedent\n",
    "                    lift = confidence / support_consequent\n",
    "                    interest = support_itemset - (support_antecedent * support_consequent)\n",
    "\n",
    "                    if lift >= min_standardized_lift:\n",
    "                        rules.append((antecedent, consequent, lift, confidence, interest))\n",
    "    return sorted(rules, key=lambda x: -x[2])\n",
    "\n",
    "rules = generate_rules(all_frequent_itemsets, support_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dba99b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 regras com lift >= 0.2:\n",
      "Antecedent: {67811000119102}, Consequent: {254632001, 162864005}, Lift: 514.7079, Confidence: 0.5265, Interest: 0.0010\n",
      "Antecedent: {67811000119102}, Consequent: {254632001, 195662009}, Lift: 514.7079, Confidence: 0.4638, Interest: 0.0009\n",
      "Antecedent: {254632001, 162864005}, Consequent: {67811000119102}, Lift: 514.7079, Confidence: 1.0000, Interest: 0.0010\n",
      "Antecedent: {254632001, 195662009}, Consequent: {67811000119102}, Lift: 514.7079, Confidence: 1.0000, Interest: 0.0009\n",
      "Antecedent: {254632001}, Consequent: {67811000119102}, Lift: 514.4791, Confidence: 0.9996, Interest: 0.0019\n",
      "\n",
      "Total de regras geradas: 86230\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTop 5 regras com lift >= 0.2:\")\n",
    "for antecedent, consequent, lift, confidence, interest in rules[:5]:\n",
    "    print(f\"Antecedent: {set(antecedent)}, Consequent: {set(consequent)}, Lift: {lift:.4f}, Confidence: {confidence:.4f}, Interest: {interest:.4f}\")\n",
    "print(\"\\nTotal de regras geradas:\", len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f01198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the frequent itemsets to a file , k2 and k3\n",
    "output_path = \"./_output/\"\n",
    "frequent_itemsets_k2 = [(set(itemset), count) for itemset, count in top_k2]\n",
    "frequent_itemsets_k3 = [(set(itemset), count) for itemset, count in top_k3]\n",
    "with open(f\"{output_path}/frequent_itemsets_k2.txt\", \"w\") as f:\n",
    "    for itemset, count in frequent_itemsets_k2:\n",
    "        f.write(f\"{itemset}, support: {count}\\n\")\n",
    "with open(f\"{output_path}/frequent_itemsets_k3.txt\", \"w\") as f:\n",
    "    for itemset, count in frequent_itemsets_k3:\n",
    "        f.write(f\"{itemset}, support: {count}\\n\")\n",
    "\n",
    "#save the rules to a file\n",
    "with open(f\"{output_path}/association_rules.txt\", \"w\") as f:\n",
    "    for ant, cons, lift, conf, interest in rules:\n",
    "        f.write(f\"Rule: {set(ant)} -> {set(cons)}, \"\n",
    "                f\"Lift: {lift:.4f}, Confidence: {conf:.4f}, Interest: {interest:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a3b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03584ed",
   "metadata": {},
   "source": [
    "# Explicação do Trabalho\n",
    "O trabalho consiste na implementação de um algoritmo Apriori para a identificação de padrões frequentes e regras de associação em um conjunto de dados médicos. O objetivo é analisar as condições médicas associadas a pacientes e gerar regras que possam indicar relações entre essas condições.\n",
    "\n",
    "\n",
    "## 1. **Leitura dos Dados**\n",
    "Comecei por carregar os dados a partir do arquivo CSV (`conditions.csv`) utilizando o Spark. O DataFrame resultante contém informações como:\n",
    "- `PATIENT`: Identificador do paciente.\n",
    "- `CODE`: Código da condição médica.\n",
    "- `DESCRIPTION`: Descrição da condição.\n",
    "\n",
    "## 2. **Agrupamento de Condições por Paciente**\n",
    "A seguir comecei por agrupar as condições médicas utilizando a função `collect_set`, que cria uma lista de transações (`patient_condition_lists`), onde cada transação representa as condições associadas a um paciente.\n",
    "\n",
    "## 3. **Definição de Parâmetros**\n",
    "Apos isso defeni os parâmetros:\n",
    "- `min_support`: Suporte mínimo absoluto (1000).\n",
    "- `min_support_ratio`: Suporte mínimo relativo, calculado como `min_support / transaction_count`.\n",
    "- `min_standardized_lift`: Lift padrao mínimo para regras de associação (0.2).\n",
    "\n",
    "## 4. **Cálculo de Itens Frequentes**\n",
    "Os itens frequentes foram identificados com base no suporte mínimo:\n",
    "- Contagem de ocorrências de cada item em todas as transações.\n",
    "- Filtragem de itens cujo suporte relativo é maior ou igual ao `min_support_ratio`.\n",
    "- Os itens frequentes de 1 elemento foram armazenados em `frequent_1_itemsets`.\n",
    "\n",
    "## 5. **Geração de Candidatos (Apriori)**\n",
    "Criei uma função `apriori_gen` para gerar candidatos de tamanho `k` a partir dos itens frequentes de tamanho `k-1`. A função combina pares de conjuntos frequentes para formar novos candidatos.\n",
    "\n",
    "\n",
    "## 6. **Identificação de Itens Frequentes para k=2 e k=3**\n",
    "Para `k=2` e `k=3`, os candidatos foram avaliados:\n",
    "- Para cada candidato, foi calculado o suporte relativo.\n",
    "- Apenas candidatos com suporte maior ou igual ao `min_support` foram considerados frequentes.\n",
    "- Os itens frequentes foram armazenados em `all_frequent_itemsets`.\n",
    "\n",
    "## 7. **Geração de Regras de Associação**\n",
    "Criei uma função `generate_rules` para gerar as regras de associação a partir dos itens frequentes:\n",
    "- Para cada itemset frequente com tamanho ≥ 2, foram geradas combinações de antecedentes e consequentes.\n",
    "- Foram calculadas métricas como confiança, lift, interesse e lift padrao.\n",
    "- Apenas regras com `standardized_lift` ≥ `min_standardized_lift` foram mantidas.\n",
    "\n",
    "## 8. **Save dos Resultados**\n",
    "Os resultados foram guardos em arquivos:\n",
    "- Itens frequentes para `k=2` e `k=3` foram salvos em arquivos de texto.\n",
    "- Regras de associação foram salvas em um arquivo separado, incluindo todas as métricas calculadas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bolsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
